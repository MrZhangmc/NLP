### ELMo (Embeddings from Language Models)
这篇文章作为ELMo论文的补充，针对网上的一些解读让我对于ELMo的理解更加细节。**其实预训练语言模型像word2vec一样会用就行，但是我认为理解这些好的论文模型设计的细节和创新点会开拓我们的视野，在进行我们自己领域的创新研究时候也许会成为思路源泉。**
本质上，之前的Word Embedding是一个静态的表示方式，所谓静态也就是指训练好的每个单词的表示就固定住了，以后使用的时候不会考虑特定场景下不同任务的上下文信息，这也是问题所在。
**这里也不会细究数学表达式，在论文中解读过。**
正如论文中说的，训练好的语言模型对于一个单词来说会获得三个embedding，分别是文章提到的pre-trained的language model用了两层的biLM获得的两个表示，还有对token进行上下文无关编码的表示，**这是通过CNN对字符级进行编码得到的特征**。
ELmo的本质是：事先用一个语言模型训练一个单词的Word Embedding，此时多义词没法区分，不过没关系。在实际使用Word Embedding的时候，单词已经具备了特定的上下文，这个时候再根据上下文去调整单词的Word Embedding表示，再经过了这样的调整后词向量就更能表达上下文的含义，从而解决了多义词的问题。**所以，ELMo本身就是根据当前上下文对Word Embedding动态调整的思路。**
