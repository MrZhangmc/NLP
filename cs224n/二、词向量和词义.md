# Introduction and Word Vectors
##介绍
- WordNet：最早阶段解释词义的工具，在NLTK中可以调取，靠人工整理，难以适应当前的词库的不断扩充，具有一定的局限性。
- 对于后来出现的one-hot编码有两点不足：
  - 当词库较大时，独热的矢量长度过长，难以有效编码。
  - 任意两个词矢量之间都是正交的，难以捕捉他们之间的相关性关系。
- 分布式语义(Distributional semantics)：出现在不同的文本语境中经常出现的意思。引出了Word vectors。

##Word vectors
词向量是一个小矢量，一个密集的向量，全为非0值组成。Word vectors有时称为**word embeddings**或者**word representations**，它们都是分布式表示。
- 词义作为一个神经词向量，能够可视化表示。

##Word2vec
Word2vec是一个模型用来学习word vectors。
下面展示Word2vec的一个实例：
![avatar](https://github.com/coderGray1296/NLP/blob/master/cs224n/pictures/1.1.png)
像例子中提到的一样，对于Word2vec模型，一个词有两个向量表示，分别是作为中心词和上下文词时的表示。例子中的上下文窗口为2，但是实际情况中如果更多地考虑上下文关联，采用的outside窗口要比这个大得多。
下图将说明具体的分布概率表示和模型的损失函数等：
![avatar](https://github.com/coderGray1296/NLP/blob/master/cs224n/pictures/1.2.png)
似然likelihood就是通过条件概率的累乘得到，我们通过优化方法来使损失函数达到最小值。下图展示了如何求得似然：
![avatar](https://github.com/coderGray1296/NLP/blob/master/cs224n/pictures/1.3.png)
![avatar](https://github.com/coderGray1296/NLP/blob/master/cs224n/pictures/1.4.png)
如上图，似然的计算方法从形式上来说很好理解，是softmax的一种表达方式。橙色部分中两个向量的点积结果表示这两个词的相似度，蓝色部分为初始化整个词典的概率分布，通过这种方式，将每个词映射到概率分布空间，成为概率。