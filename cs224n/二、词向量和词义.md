# Word Vectors and word senses
##损失函数的优化方法
#####批量梯度下降(Gradient Descent):
批量梯度下降的表达如下图: ![avatar](https://github.com/coderGray1296/NLP/blob/master/cs224n/pictures/2.1.png)
其中的J就是再上一节中定义的损失函数：
![avatar](https://github.com/coderGray1296/NLP/blob/master/cs224n/pictures/2.2.png) 
可以看到J是关于所有t的损失函数，因此在进行批量梯度下降时，每一次更新权重都要计算一遍所有的windows，再更新。
- 优点：
  -  一次迭代是对所有样本进行计算，此时利用矩阵进行操作，实现了并行。
  -  由全数据集确定的方向能够更好地代表样本总体，从而更准确地朝向极值所在的方向。当目标函数为凸函数时，优化算法一定能够得到全局最优。
- 缺点：当样本数目(这里的问题就是vocab的window很多)很大时，每迭代一步都需要对所有样本计算，训练过程会很慢。

**因此根据批量梯度下降的缺点，提出了随机梯度下降优化算法(SGD)，事实证明这个算法是双赢的，虽然估计极为粗糙，只针对其中一个词和其上下文关联的词的参数，看不到模型中绝大部分的参数，不能保证朝着最小化的方向前进。但是在实际使用中，这个梯度估计尤其宝贵，相比BGD要快上几个数量级，而且神经网络算法喜欢噪声，有助于神经网络的学习。**

#####随机梯度下降(Stochastic Gradient Descent)
在当前问题中的定义为：我们只选取文中的一个位置，得到了一个中心词和它周围的词（上下文词），每次移动一个位置，对参数求解梯度，用这个梯度估计值更新权重。
- 优点：由于不是在全部训练数据上的损失函数，而是在每轮迭代中，随机优化某一条训练数据上的损失函数，这样每一轮参数的更新速度大大加快。
- 缺点：
  -  准确度下降。由于即使在目标函数为强凸函数的情况下，SGD仍旧无法做到线性收敛。
  -  可能会收敛到局部最优，由于单个样本并不能代表全体样本的趋势。
  -  不易并行实现。