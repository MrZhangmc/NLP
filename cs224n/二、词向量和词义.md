# Word Vectors and word senses
##损失函数的优化方法
#####批量梯度下降(Gradient Descent):
批量梯度下降的表达如下图: ![avatar](https://github.com/coderGray1296/NLP/blob/master/cs224n/pictures/2.1.png)
其中的J就是再上一节中定义的损失函数：
![avatar](https://github.com/coderGray1296/NLP/blob/master/cs224n/pictures/2.2.png) 
可以看到J是关于所有t的损失函数，因此在进行批量梯度下降时，每一次更新权重都要计算一遍所有的windows，再更新。
- 优点：
  -  一次迭代是对所有样本进行计算，此时利用矩阵进行操作，实现了并行。
  -  由全数据集确定的方向能够更好地代表样本总体，从而更准确地朝向极值所在的方向。当目标函数为凸函数时，优化算法一定能够得到全局最优。
- 缺点：当样本数目(这里的问题就是vocab的window很多)很大时，每迭代一步都需要对所有样本计算，训练过程会很慢。

**因此根据批量梯度下降的缺点，提出了随机梯度下降优化算法(SGD)，事实证明这个算法是双赢的，虽然估计极为粗糙，只针对其中一个词和其上下文关联的词的参数，看不到模型中绝大部分的参数，不能保证朝着最小化的方向前进。但是在实际使用中，这个梯度估计尤其宝贵，相比BGD要快上几个数量级，而且神经网络算法喜欢噪声，有助于神经网络的学习。**

#####随机梯度下降(Stochastic Gradient Descent)
在当前问题中的定义为：我们只选取文中的一个位置，得到了一个中心词和它周围的词（上下文词），每次移动一个位置，对参数求解梯度，用这个梯度估计值更新权重。
![avatar](https://github.com/coderGray1296/NLP/blob/master/cs224n/pictures/2.3.png)
- 优点：由于不是在全部训练数据上的损失函数，而是在每轮迭代中，随机优化某一条训练数据上的损失函数，这样每一轮参数的更新速度大大加快。
- 缺点：
  -  准确度下降。由于即使在目标函数为强凸函数的情况下，SGD仍旧无法做到线性收敛。
  -  可能会收敛到局部最优，由于单个样本并不能代表全体样本的趋势。
  -  不易并行实现。

##Word2Vec中的模型
Word2vec中主要有两种模型，分别是Skip-Gram和CBOW模型。
- Skip-Gram：给定input word来预测上下文。
- CBOW：给定上下文，预测input word。
![avatar](https://github.com/coderGray1296/NLP/blob/master/cs224n/pictures/2.4.png)
在课程中只考虑Skip-Gram，通过移动窗口，在每一个窗口中利用中心词来预测上下文词的概率。
##获取word vectors
**1.通过SVD分解共现矩阵捕捉词向量**
共现矩阵如下：
![avatar](https://github.com/coderGray1296/NLP/blob/master/cs224n/pictures/2.5.png)
然后通过奇异值分解对共现矩阵进行分解，将得到的正交矩阵作为词的词向量。
![avatar](https://github.com/coderGray1296/NLP/blob/master/cs224n/pictures/2.6.png)
![avatar](https://github.com/coderGray1296/NLP/blob/master/cs224n/pictures/2.7.png)
除此之外还存在其他方法，不过大部分都是基于共现矩阵和奇异值分解(SVD)来实现的。这里统称为**Count based计数方法**
**2.直接预测来获得词向量**
下面对比这两者获取方式的优缺点。
![avatar](https://github.com/coderGray1296/NLP/blob/master/cs224n/pictures/2.8.png)
##GloVe
通过综合以上方法中的优点，提出了GloVe模型，也称为全局向量(Global Vectors)模型。
![avatar](https://github.com/coderGray1296/NLP/blob/master/cs224n/pictures/2.9.png)
Skip-Gram模型试图一次捕获共现的一个窗口，GloVe试图捕捉这些单词词频的总体统计数据。

