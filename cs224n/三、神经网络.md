# Word Window classification and Neural Network
##使用softmax和(cross-entropy)交叉熵进行训练
我们现在考虑分类问题，假设x是给定的训练数据，那么我们只需要最大化x得到y的概率即可进行归类。我们使用之前提到过的softmax形式：
![avatar]()
下面将损失函数的形式定义为交叉熵，其中的q就是softmax的值，而p为向量的one-hot编码，所以只有一个元素为1，其余皆为0，因而，最终的交叉熵定义就是所有样本对应的损失函数的累加。
![avatar]()
除此之外，很有必要在整个交叉熵的损失函数中加入**正则项**。正则能够防止有很多特征时模型的过拟合现象。
