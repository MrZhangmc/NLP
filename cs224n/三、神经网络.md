# Word Window classification and Neural Network
##使用softmax和(cross-entropy)交叉熵进行训练
我们现在考虑分类问题，假设x是给定的训练数据，那么我们只需要最大化x得到y的概率即可进行归类。我们使用之前提到过的softmax形式：
![avatar](https://github.com/coderGray1296/NLP/blob/master/cs224n/pictures/3.1.png)
下面将损失函数的形式定义为交叉熵，其中的q就是softmax的值，而p为向量的one-hot编码，所以只有一个元素为1，其余皆为0，因而，最终的交叉熵定义就是所有样本对应的损失函数的累加。
![avatar](https://github.com/coderGray1296/NLP/blob/master/cs224n/pictures/3.2.png)
除此之外，很有必要在整个交叉熵的损失函数中加入**正则项**。正则能够防止有很多特征时模型的过拟合现象。
![avatar](https://github.com/coderGray1296/NLP/blob/master/cs224n/pictures/3.3.png)

##re-training word vectors
在我们的任务中，词向量通常使用之前接触过的Word2Vec或者GloVe等预训练好的模型，当时对于不同的问题，我们需要在各种情景的语境下，重新训练词向量以很好地进行我们的训练任务。但是这一步骤并不是必须的：
- 当我们有小的训练数据集，我们不要训练词向量，固定使用预训练模型所提供的词向量。
- 当我们有很大的训练数据集，我们可以训练词向量以更好地描述任务，将词向量作为总体目标的一部分来训练它们。

##Word Window Classification
一般我们不对单独的单词进行分类，而是要结合上下文语境、语法和单词之间的关联性等，进行分类。
####窗口分类模型(Window Classification)
将命名实体分为4类：人名、地名、组织名和其他。
**训练目标** 训练一个softmax分类器，通过给中心词分配一个标签，同时将窗口内在中心词周围的词连接起来。
![avatar](https://github.com/coderGray1296/NLP/blob/master/cs224n/pictures/3.5.png)
将拼接后的结果作为模型新的输入，进行训练。
