# Word Window classification and Neural Network
## 使用softmax和(cross-entropy)交叉熵进行训练
我们现在考虑分类问题，假设x是给定的训练数据，那么我们只需要最大化x得到y的概率即可进行归类。我们使用之前提到过的softmax形式：
![avatar](https://github.com/coderGray1296/NLP/blob/master/cs224n/pictures/3.1.png)
下面将损失函数的形式定义为交叉熵，其中的q就是softmax的值，而p为向量的one-hot编码，所以只有一个元素为1，其余皆为0，因而，最终的交叉熵定义就是所有样本对应的损失函数的累加。
![avatar](https://github.com/coderGray1296/NLP/blob/master/cs224n/pictures/3.2.png)
除此之外，很有必要在整个交叉熵的损失函数中加入**正则项**。正则能够防止有很多特征时模型的过拟合现象。
![avatar](https://github.com/coderGray1296/NLP/blob/master/cs224n/pictures/3.3.png)

## re-training word vectors
在我们的任务中，词向量通常使用之前接触过的Word2Vec或者GloVe等预训练好的模型，当时对于不同的问题，我们需要在各种情景的语境下，重新训练词向量以很好地进行我们的训练任务。但是这一步骤并不是必须的：
- 当我们有小的训练数据集，我们不要训练词向量，固定使用预训练模型所提供的词向量。
- 当我们有很大的训练数据集，我们可以训练词向量以更好地描述任务，将词向量作为总体目标的一部分来训练它们。

## Word Window Classification
一般我们不对单独的单词进行分类，而是要结合上下文语境、语法和单词之间的关联性等，进行分类。
#### 窗口分类模型(Window Classification)
将命名实体分为4类：人名、地名、组织名和其他。
**训练目标** 训练一个softmax分类器，通过给中心词分配一个标签，同时将窗口内在中心词周围的词连接起来。
![avatar](https://github.com/coderGray1296/NLP/blob/master/cs224n/pictures/3.5.png)
将拼接后的结果作为模型新的输入，进行训练。
**之前训练词向量，每个词有两个向量u和v，训练完成后，将u+v作为这个词的最终的词向量表示。因此每个词都只有一个词向量，以后的问题也都默认只有一个词向量。（包括在窗口分类模型中使用的词向量）**

## 最大间距损失函数
相比于softmax的交叉相误差更具有鲁棒性也更加强大。
![avatar](https://github.com/coderGray1296/NLP/blob/master/cs224n/pictures/3.6.png)
让想要的以中心词为核心的窗口得分尽可能高，不想要窗口尽可能低。比如例子中想要以地点为中心词的窗口。

**通过使用SGD优化训练参数，选择更小的窗口进行训练，会增大随机性从而避免陷入局部最优。**