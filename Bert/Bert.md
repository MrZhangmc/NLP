### Bidirectional Encoder Representations from Transformers(BERT)
#### 摘要：
Bert用来从两个方向预训练未标记文本的**深度双向表示**。所以Bert可以被fine-tuning只用一个额外的输出层，就可以使用多种任务，并且不需要改变大的架构。
#### 介绍：
现有两种策略可用于将预训练语言表示应用于下游任务：基于特征和微调。 基于特征的方法，如ELMo（Peters等，2018a），使用特定于任务的体系结构，其中包括预先训练的表示作为特征。 微调方法，例如**Generative Pre-trained Transformer**，引入了特殊任务的参数，并通过简单微调所有预先训练的培训下游任务参数。 这两种方法在预训练期间共享相同的目标函数，在这些方法中，他们使用单向语言模型来学习一般语言表示。
在GPT中，通过单向传递令牌这种方式限制了预训练期间可以使用的架构的选择，同时也限制了预训练语言表示的能力。
Bert缓解了这种局限性，通过使用**masked language model**(MLM)预训练目标。**MLM**从输入中随机地遮盖一些标记，并且目标是仅基于上下文来预测被遮盖单词的原始id。同时MLM目标能够使得语言表示融合左右的上下文信息。除此之外，还使用了**next sentence prediction**预训练文本语言的表示。
#### BERT
框架有两个步骤：预训练和微调。在预训练期间，该模型在不同的预训练任务上训练未标记的数据。 对于微调，首先使用预先训练的参数初始化BERT模型，并使用来自下游任务的标记数据对所有参数进行微调。每个下游任务都具有独立的微调模型，即使它们使用相同的预先训练的参数进行初始化。 下图为问答任务的示例。
![avatar](https://github.com/coderGray1296/NLP/blob/master/Bert/pictures/1.png)
Bert的显著特点就是它在不同任务中的统一构架，预训练的架构与最终特定下游任务的架构之间存在着微小的差异。
Bert的模型架构是一个多层双向Transformer编码器。
###### 输入/输出表示
为了使BERT处理各种下游任务，我们的输入表示能够在一个获取序列中明确地表示单个句子和一对句子（例如，⟨问题，答案⟩）。 在整个这项工作中，“句子”可以是连续文本的任意跨度，而不是实际的语言句子。“序列”指的是BERT的输入标记序列，它可以是单个句子或两个句子打包在一起。
对于给定的标记，其输入表示通过对相应的标记，段和位置嵌入求和来构造。 可以在下图中看到这种结构的可视化。
![avatar](https://github.com/coderGray1296/NLP/blob/master/Bert/pictures/2.png)
##### 预训练阶段
不使用从左到右或者从右到左的语言模型来预训练，而是使用两个非监督任务来进行预训练。对应于图1中的左边部分。
- Task1 Masked LM：标准的条件概率语言模型只能单方向进行预测，尽管从逻辑上来说，双向模型的效果要好于单向。因此，为了训练一个深度双向表示，我们按照一定比例随机遮盖输入tokens，然后预测这些被遮盖的tokens，这个过程称为**masked LM**(MLM)。与去噪自编码器相反，只预测被遮盖的序列而不是整个输入。这样虽然能够获得双向预训练模型，但是由于在fine-tuning过程中被遮盖的tokens不会出现，会导致预训练过程和fine-tuning过程不匹配。因此不实际替换mask tokens，而是随机选择位置进行预测。
- Task2 Next Sentence Prediction(NSP)：理解两个句子之间的关系对于问答和自然语言推理等下游任务来说很重要。为了训练理解句子关系的模型，预先训练一个下一个句子预测任务(NSP)。
##### Fine-tuning阶段
Transformer中的self-attention允许Bert通过交换适当的输入和输出来模拟许多下游任务，无论它们是单文本还是文本对。
对于每项任务，只需要将特定任务的输入和输出插入到Bert中，并且端到端地对所有参数进行微调。与预训练相比，微调相对成本较低。
**三种预训练模型的对比**
![avatar](https://github.com/coderGray1296/NLP/blob/master/Bert/pictures/3.png)
**不同任务微调对比**
![avatar](https://github.com/coderGray1296/NLP/blob/master/Bert/pictures/4.png)
